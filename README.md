# ðŸ“š Paper Notebook
Welcome! This repo contains summaries and notes for papers I've read and a to-read list for upcoming work. I focus on topics in:
- Hallucination Detection for Large Language Models(LLMs)
- Hallucination Mitigation for Multimodal Large Language Models(MLLMs)
- Safety alignment in both LLMs and MLLMs

## About Me ðŸ”†
I am an incoming first-year Ph.D. student at the State Key Laboratory of Pattern Recognition, University of Chinese Academy of Sciences (UCAS), starting in September 2025, advised by Prof. [Liang Wang](https://people.ucas.ac.cn/~wangliang) and [Qiang Liu](https://people.ucas.ac.cn/~qiangliu).
ðŸ“« Find me here:
- [Zhihu](https://www.zhihu.com/people/serein-77-71-14)

## ðŸ—‚ Structure
```
ðŸ“¦ hallucination-and-alignment-papers
â”œâ”€â”€ README.md
â”œâ”€â”€ to_read.md  --- A prioritized list of papers to read
â”œâ”€â”€ templates
â”‚   â””â”€â”€ paper_note_template.md  
â””â”€â”€ papers
    â”œâ”€â”€ LLM
    â”‚   â”œâ”€â”€ hallucination_detection.md
    â”‚   â””â”€â”€ safety_alignment.md
    â””â”€â”€ MLLM
        â”œâ”€â”€ hallucination_mitigation.md
        â””â”€â”€ safety_alignment.md
```
